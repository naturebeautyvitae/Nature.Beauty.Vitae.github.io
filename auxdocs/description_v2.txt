Artificial intelligence is everywhere. From its use in medical diagnosis to relationship chatbots, AI technology is improving rapidly in the diverse tasks it can perform, offering genuine benefits to human social life along with novel risks. With so much at stake, AI designers and users would be wise to have a sober conversation about what AI systems are. In addition to technical knowledge of AI systems, such a conversation also requires a strong theoretical understanding of AI systems as unique agents that encode, or can be interpreted as encoding, intentional actions in communication with humans. In particular, large language models (LLMs) are often used by humans in unique forms of request-making, collaboration, and problem solving. As a result, humans find themselves working with and alongside LLM systems when carrying out a range of tasks in research, industry, art, and more. It is the goal of this proposed conference, "Agency and Intentions in Artificial Intelligence," to promote a much-needed interdisciplinary conversation surrounding the theoretical basis of AI systems as agent-like collaborators.  
 
We plan to bring together philosophers, linguists, cognitive scientists, and computer scientists in a rich and multi-faceted discussion regarding questions of agency and intentions in AI and LLMs. The proposed workshop is interdisciplinary in nature, since AI systems are themselves the products of multiple disciplines. Rather than view the complexity of AI as a challenge to productive conversation, we think of it as an opportunity to bring thinkers from diverse backgrounds together to share various tools, methods, theories, and perspectives on how to make sense of agency in non-human computational systems. We do not expect all of the presenters at the workshop to share the same methodological assumptions or research backgrounds, nor do we expect such congruence in our attendees. This allows all participants to benefit from seeing questions of agency and AI from new standpoints. Additionally, it encourages speakers and attendees to present their ideas and questions in clear and accessible ways so that, say, a linguist can effectively communicate their work to philosophers and cognitive scientists.
 
"Agency and Intentions in Artificial Intelligence" (AIAI) builds on the success of our workshop series "Agency and Intentions in Language," which is in its fourth iteration. Given the success of "Agency and Intentions in Language," which covers broader intersections between linguistics and philosophy, we are confident that AIAI will be a productive and thought-provoking discussion across multiple academic disciplines. Some questions and topics that presenters might investigate are as follows:
 
-Are AI systems, or LLMs in particular, unique kinds of agents? How should we understand the human propensity to treat them as such? Do AI systems and LLMs produce linguistic outputs that can be understood through the concepts of "intentional action" or "intentions"?
-Are AI systems or LLMs unique language users? How can we best study, discuss, and engage with their linguistic outputs?
-What semantic properties can be attributed to outputs from LLMs or other AI systems?
-What do LLMs teach us about concepts themselves, specifically those related to agency, such as "intentions" and "decision-making" and "reasons" and "judgment"? Are there fundamental differences in the way "intentional action" is captured in human language use as compared to how it is captured in LLMs?
-Are LLMs participating in acts and expressions in similar ways to human agents? For example, do LLMs encode for something like an “understanding” of concepts? Do they “refer” to things and ideas in their linguistic outputs? Are they “responding” to human requests and inquiries?
-Are LLM concept vectors sufficiently grounded—i.e., are they connected in the right ways to the real world—to constitute certain semantic properties that human expressions possess?
-How are specific ethical problems related to AI informed by the above questions about the linguistic capacities of AI systems? How might those ethical issues be better addressed?
-Can cognitive scientific models of human thinking, agency, and decision-making benefit from studying LLMs? What can cognitive science tell us how about how LLMs “process” information?
-How do computer scientists think about the role of agency and intentions when developing LLMs?
 
The heart of the questions above, which are only a small fraction of those that speakers might address at AIAI, is a drive to learn and discover more about AI systems as potential agents and decision-makers. While the workshop is not directly focused on providing solutions to ethical problems in AI development, questions of ethics and moral responsibility both motivate the discussion and will be included in the workshop. What AIAI will uniquely achieve, though, is an interdisciplinary conversation about the technical philosophical and linguistic features of the very AI systems that humans will continue to employ in ever more domains of social life. A new phase AI is here, and we think this offers new opportunities and challenges for people in all areas of life. Our goal is to meet these opportunities and challenges through the unique theoretical perspectives offered by linguists, philosophers, computer scientists, and cognitive scientists. After all, it is impossible to take practical moral action in response to AI systems if we cannot make sense of what AI is, does, or intends.
